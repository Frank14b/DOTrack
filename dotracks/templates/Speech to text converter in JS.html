<!DOCTYPE html>
<html>

<head>
	<title>Speech to text converter in JS</title>
	<link rel="stylesheet" type="text/css" href="https://cdnjs.cloudflare.com/ajax/libs/font-awesome/4.6.1/css/font-awesome.min.css"
	/>
	<style type="text/css">
		body {
			font-family: verdana;
		}

		#result {
			height: 200px;
			border: 1px solid #ccc;
			padding: 10px;
			box-shadow: 0 0 10px 0 #bbb;
			margin-bottom: 30px;
			font-size: 14px;
			line-height: 25px;
		}

		button {
			font-size: 20px;
			position: absolute;
			top: 240px;
			left: 50%;
		}
	</style>
</head>

<body>
	<h4 align="center">Speech to text converter in JS</h4>
	<div id="result"></div>
	<button onclick="startConverting();">
		<i class="fa fa-microphone"></i>
	</button>

	<p>Volume</p>
	<input id="volume" type="range" min="0" max="1" step="0.1" value="0.5"/>
	

	<script type="text/javascript">

		var r = document.getElementById('result');

		function startConverting() {
			if ('webkitSpeechRecognition' in window) {
				var speechRecognizer = new webkitSpeechRecognition();
				speechRecognizer.continuous = true;
				speechRecognizer.interimResults = true;
				speechRecognizer.lang = 'fr-FR';
				speechRecognizer.start();

				var finalTranscripts = '';

				speechRecognizer.onresult = function (event) {
					var interimTranscripts = '';
					for (var i = event.resultIndex; i < event.results.length; i++) {
						var transcript = event.results[i][0].transcript;
						transcript.replace("\n", "<br>");
						if (event.results[i].isFinal) {
							finalTranscripts += transcript;
						} else {
							interimTranscripts += transcript;
						}
					}
					r.innerHTML = finalTranscripts + '<span style="color:#999">' + interimTranscripts + '</span>';
				};
				speechRecognizer.onerror = function (event) {
				};
			} else {
				alert('Your browser is not supported. If google chrome, please upgrade!')
				r.innerHTML = 'Your browser is not supported. If google chrome, please upgrade!';
			}
		}
	</script>

	<script type="text/javascript">


		var webaudio_tooling_obj = function () {

			var audioContext = new AudioContext();

			console.log("audio is starting up ...");

			var BUFF_SIZE = 16384;

			var audioInput = null,
				microphone_stream = null,
				gain_node = null,
				script_processor_node = null,
				script_processor_fft_node = null,
				analyserNode = null;

			if (!navigator.getUserMedia)
				navigator.getUserMedia = navigator.getUserMedia || navigator.webkitGetUserMedia ||
					navigator.mozGetUserMedia || navigator.msGetUserMedia;

			if (navigator.getUserMedia) {

				navigator.getUserMedia({ audio: true },
					function (stream) {
						start_microphone(stream);
					},
					function (e) {
						alert('Error capturing audio.');
					}
				);

			} else { alert('getUserMedia not supported in this browser.'); }

			// ---

			function show_some_data(given_typed_array, num_row_to_display, label) {

				var size_buffer = given_typed_array.length;
				var index = 0;
				var max_index = num_row_to_display;

				console.log("__________ " + label);

				for (; index < max_index && index < size_buffer; index += 1) {

					console.log(given_typed_array[index]);
				}
			}

			function process_microphone_buffer(event) {

				var i, N, inp, microphone_output_buffer;

				microphone_output_buffer = event.inputBuffer.getChannelData(0); // just mono - 1 channel for now

				// microphone_output_buffer  <-- this buffer contains current gulp of data size BUFF_SIZE

				show_some_data(microphone_output_buffer, 5, "from getChannelData");
			}

			function start_microphone(stream) {

				gain_node = audioContext.createGain();
				gain_node.connect(audioContext.destination);

				microphone_stream = audioContext.createMediaStreamSource(stream);
				microphone_stream.connect(gain_node);

				script_processor_node = audioContext.createScriptProcessor(BUFF_SIZE, 1, 1);
				script_processor_node.onaudioprocess = process_microphone_buffer;

				microphone_stream.connect(script_processor_node);

				// --- enable volume control for output speakers

				document.getElementById('volume').addEventListener('change', function () {

					var curr_volume = this.value;
					gain_node.gain.value = curr_volume;

					console.log("curr_volume ", curr_volume);
				});

				// --- setup FFT

				script_processor_fft_node = audioContext.createScriptProcessor(2048, 1, 1);
				script_processor_fft_node.connect(gain_node);

				analyserNode = audioContext.createAnalyser();
				analyserNode.smoothingTimeConstant = 0;
				analyserNode.fftSize = 2048;

				microphone_stream.connect(analyserNode);

				analyserNode.connect(script_processor_fft_node);

				script_processor_fft_node.onaudioprocess = function () {

					// get the average for the first channel
					var array = new Uint8Array(analyserNode.frequencyBinCount);
					analyserNode.getByteFrequencyData(array);

					// draw the spectrogram
					if (microphone_stream.playbackState == microphone_stream.PLAYING_STATE) {

						show_some_data(array, 5, "from fft");
					}
				};
			}

		}(); //  webaudio_tooling_obj = function()
	</script>
</body>

</html>